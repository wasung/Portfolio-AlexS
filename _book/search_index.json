[["index.html", "Portfolio DSFB 1 Introduction 1.1 Contact", " Portfolio DSFB 1 Introduction welcome to my Data Science for Biology portfolio! My name is Alex Sung, and I am a life scientist with a interest in applying data science techniques to solve biological problems.I am currently pursuing a Bachelor’s degree in Life Science with a specialization in Biomolecular Research at HU University of Applied Sciences in Utrecht, the Netherlands. Additionally, I am minoring in Data Science for Biology, which has allowed me to explore the intersection of life sciences and computational methods. The course is created by … This github page will serve as my portfolio as data scientist. 1.1 Contact email: wkasung@gmail.com GitHub: https://github.com/wasung "],["curriculum-vitae-wing-kien-alex-sung.html", "2 Curriculum Vitae Wing-kien Alex Sung Education Work Experience Activities Additional Skills", " 2 Curriculum Vitae Wing-kien Alex Sung Address: De Kriek 29, Vleuten E-mail: wkasung@gmail.com Phone: +31 6 52220022 Date of Birth: 24 September 2000 Nationality: Dutch Education 2020 – Present HU University of Applied Sciences, Utrecht, the Netherlands BSc Life Science Biomolecular Research specialisation Minor: Data Science for Biology Honours Programme Extra year due to sports injury 2012 – 2018 Niftarlake College, Maarssen, the Netherlands General Secondary Education (upper-intermediate level) Work Experience Jul 2024 – Present Cell and Gene Therapy Facility Intern, Prinses Maxima Centrum voor Kinderoncologie, Utrecht Environmental monitoring and PQ of cleanroom under ISO and GMP guidelines. Oct 2023 – Present Founder, Kokoro Skincare, Rotterdam Founded Kokoro Skincare, managing strategy, compliance, and marketing. Expanded to Germany and Belgium, building a trusted Japanese beauty e-commerce brand. Dec 2023 – Present Freelance Business Analyst, Remote Streamlined reporting processes and translated data into user-friendly reports. Supported stakeholders in data-driven decision-making. Feb 2022 – Nov 2022 Laboratory Technician, Saltro BV, Utrecht Executed precise analysis of COVID-19 samples using rt-qPCR techniques. Jul 2021 – Dec 2021 Laboratory Technician, Eurofins, Barneveld Conducted soil and water pretreatment for ICP-MS analysis. Measured diverse environmental parameters. Activities Sep 2023 – Present Head of Partnership Committee, CSA-EUR, Rotterdam Sep 2023 – Jun 2024 Active Member, Delft Committee, CSA-EUR, Rotterdam Jan 2020 – Present Sponsored Badminton Player, Victor Europe GmbH, Germany Additional Skills Languages: English (Professional), Dutch (native) Software &amp; Tools: Microsoft Office (Outlook, Word, Excel, PowerPoint) Power BI (good command) IBM SPSS Statistics (fair command) RStudio (fair command) Driving License: Full Hobbies: Fitness, Badminton, Reading, Photography "],["looking-ahead.html", "3 Looking Ahead Professional Development", " 3 Looking Ahead My academic journey began with a fascination for molecular biology, but over time, I’ve found myself drawn to a broader vision of how I want to contribute to the life sciences sector. Rather than remaining solely in the laboratory, I’m increasingly motivated by the idea of playing a more strategic role—where science, data, and business come together to create real-world impact. I want to be involved in designing solutions, informing decisions, and shaping the direction of healthcare innovations, particularly in fields where science meets patient outcomes and commercial viability. To support this shift, I chose to pursue the Data Science for Biology minor alongside my Bachelor’s in Life Sciences. This combination reflects my desire to bridge hard scientific knowledge with analytical and business-oriented thinking. I believe that fluency in data—whether it’s understanding experimental design, visualizing trends, or interpreting statistical outcomes—is a crucial skill in almost any role across life sciences, biotech, and healthcare consulting. From my perspective, knowing how to work with data empowers you to be a decision-maker. While my initial goal was to find an internship specifically focused on data science or sequencing, I ultimately accepted a position at a cell and gene therapy facility—an opportunity that has proven both challenging and eye-opening. Working in this highly regulated environment has taught me a great deal about Good Manufacturing Practice (GMP), cleanroom operations, and contamination control strategies. I’ve been involved in environmental monitoring and performance qualification (PQ) processes, all of which have given me a deeper appreciation for the complexities of translating research into safe therapies. While this internship doesn’t involve coding or computational analysis directly, it’s reinforcing my understanding of the operational and regulatory backbone of modern therapeutics, something I consider just as important. Looking further ahead, I see strategy consulting as the most likely next step in my career. I’m especially interested in specializing within the healthcare and life sciences sector, where I can apply my background to strategic challenges in pharma, biotech, or health tech. However, I remain open and excited to explore other industries as well, particularly during the early stages of my consulting journey. I value the opportunity to learn across a wide range of business contexts and expand my thinking beyond my academic background. To prepare for this next phase, I’ve applied to the pre-master programme at Erasmus University Rotterdam, with the goal of continuing on to a Master in Management. This academic step will allow me to deepen my understanding of business, strategy, and organizational dynamics, and will complement the technical and scientific knowledge I’ve built during my Life Sciences degree. In short, I’m still evolving as a professional, but I’m actively shaping my own trajectory. I’m seeking out the tools, experiences, and mindsets that will help me grow—and I’m committed to building a career that blends the depth of science with the breadth of strategy. Professional Development As part of the Data Science for Biology course, we are allocated time to independently explore a bioinformatics-related topic of our choice. I’ve chosen to focus on variant calling, specifically using the Variant Tools package available via Conda, as this aligns with my interest in genomics and data-driven biological insights. I plan to apply these skills in a self-directed project using a publicly available dataset, allowing me to gain hands-on experience beyond the classroom or lab setting. Outside of this academic framework, I’m continuing to build up my data skills independently. I’ve been exploring tools like Power BI to strengthen my business intelligence capabilities, and I’m actively learning more about Python for data analysis. I believe that by combining my scientific background with growing analytical and technical expertise, I’ll be well-positioned for cross-disciplinary roles — whether in consulting, biotech strategy, or data-driven innovation within the healthcare sector. "],["guerilla-analytics.html", "4 Guerilla Analytics", " 4 Guerilla Analytics Need to run the data at home through the pipe and clean up the folders still ( will use projecticum folders). ## Assignment Overview In this assignment, the goal is to apply the Guerrilla Analytics framework to structure and document your project in a reproducible, collaborative, and future-proof way. The focus is on organizing both: - The DAUR-II project (RNAseq &amp; metagenomics analysis), and - The Projecticum (ongoing group project in DSFB2) 4.0.1 1. Reorganize Your DAUR-II Project Locate RStudio project folder used for the DAUR-II assignments. Download your work without the heavy data files Restructure the folder according to the Guerrilla Analytics folder principles 4.0.1.1 Data folders should contain: Only a README.txt file The README.txt should include: Description of the dataset Data format Where the data is stored (path or location) Any preprocessing steps or notes 4.0.2 2. Apply to Your RNAseq and Metagenomics Work You previously worked with: 3 RNAseq datasets One of them was the exam project 2 Metagenomics datasets For each dataset: - Create a modular folder (e.g., Data001, Data002, etc.) - Add a README.txt inside each module - Add any import or checksum scripts to a supporting/ subfolder 4.0.3 3. Use the {fs} Package to Generate a Folder Tree Screenshot To showcase your new project structure: library(fs) # Print your current directory tree (recursively) dir_tree(&quot;guerrilla-project&quot;, recurse = TRUE) ## guerrilla-project ## ├── 0_admin ## │ └── README.md ## ├── 1_data_raw ## │ ├── Data001 ## │ │ ├── 2024-01-01_cleanroom_counts.csv ## │ │ ├── 2024-01-01_md5sum_cleanroom_counts.md5 ## │ │ ├── README.txt ## │ │ ├── supporting ## │ │ │ ├── cleanroom_import.R ## │ │ │ └── md5sum.R ## │ │ └── v01 ## │ │ ├── 2023-12-15_cleanroom_counts.csv ## │ │ └── 2023-12-15_md5sum_cleanroom_counts.md5 ## │ ├── Data002 ## │ │ ├── experimental_design_notes.xlsx ## │ │ └── README.txt ## │ └── README.txt ## ├── 2_data_clean ## │ └── cleaned_data_sample.csv ## ├── 3_analysis ## │ ├── 01_import_and_clean.R ## │ ├── 02_analysis.R ## │ └── 03_visualizations.R ## ├── 5_meta ## │ ├── data_dictionary.md ## │ └── variable_definitions.md ## ├── 6_log ## │ └── run_log.md ## └── guerrilla_project.Rproj "],["c.elegans-plate-experiment.html", "5 C.elegans Plate Experiment 5.1 Exploring the Excel Dataset Structure 5.2 Importing the Dataset into R 5.3 Checking and Correcting Data Types 5.4 Visualizing Raw Data: Scatterplot 5.5 Fixing Axis Label Issues in ggplot 5.6 Identifying Positive Controls 5.7 Identifying Negative Controls 5.8 Designing a Statistical Analysis Strategy 5.9 Normalizing Data Using the Control Group 5.10 Why Normalize?", " 5 C.elegans Plate Experiment 5.1 Exploring the Excel Dataset Structure “Review the following Excel file in ./data/CE.LIQ.FLOW.062_Tidydata.xlsx…” The Parameters tab is unclear — it’s difficult to interpret what each column means. The Raw Data column lacks explanation, and the purpose of the coloring (particularly the long pink section) is not described. Other tabs are even more confusing due to minimal labeling and inconsistent formatting. 5.2 Importing the Dataset into R “Open the file in R using the {readxl} package.” library(here) library(readxl) library(tidyverse) # Import data CElegans_data &lt;- read_excel(here::here(&quot;Data&quot;, &quot;CE.LIQ.FLOW.062_Tidydata.xlsx&quot;)) # Preview head(CElegans_data) ## # A tibble: 6 × 34 ## plateRow plateColumn vialNr dropCode expType expReplicate expName expDate expResearcher expTime expUnit expVolumeCounted RawData compCASRN compName ## &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 NA NA 1 a experiment 3 CE.LIQ… 2020-11-30 00:00:00 Sergio Reijn… 68 hour 50 44 24157-81… 2,6-dii… ## 2 NA NA 1 b experiment 3 CE.LIQ… 2020-11-30 00:00:00 Sergio Reijn… 68 hour 50 37 24157-81… 2,6-dii… ## 3 NA NA 1 c experiment 3 CE.LIQ… 2020-11-30 00:00:00 Sergio Reijn… 68 hour 50 45 24157-81… 2,6-dii… ## 4 NA NA 1 d experiment 3 CE.LIQ… 2020-11-30 00:00:00 Sergio Reijn… 68 hour 50 47 24157-81… 2,6-dii… ## 5 NA NA 1 e experiment 3 CE.LIQ… 2020-11-30 00:00:00 Sergio Reijn… 68 hour 50 41 24157-81… 2,6-dii… ## 6 NA NA 2 a experiment 3 CE.LIQ… 2020-11-30 00:00:00 Sergio Reijn… 68 hour 50 35 24157-81… 2,6-dii… ## # ℹ 19 more variables: compConcentration &lt;chr&gt;, compUnit &lt;chr&gt;, compDelivery &lt;chr&gt;, compVehicle &lt;chr&gt;, elegansStrain &lt;chr&gt;, elegansInput &lt;dbl&gt;, ## # bacterialStrain &lt;chr&gt;, bacterialTreatment &lt;chr&gt;, bacterialOD600 &lt;dbl&gt;, bacterialConcX &lt;dbl&gt;, bacterialVolume &lt;dbl&gt;, bacterialVolUnit &lt;chr&gt;, ## # incubationVial &lt;chr&gt;, incubationVolume &lt;dbl&gt;, incubationUnit &lt;chr&gt;, incubationMethod &lt;chr&gt;, incubationRPM &lt;dbl&gt;, bubble &lt;lgl&gt;, incubateTemperature &lt;dbl&gt; 5.3 Checking and Correcting Data Types compConcentration was imported as character and was converted to numeric. compName and expType were converted to factors to allow proper grouping and visualization. # Inspect column types str(CElegans_data %&gt;% select(RawData, compName, compConcentration)) ## tibble [360 × 3] (S3: tbl_df/tbl/data.frame) ## $ RawData : num [1:360] 44 37 45 47 41 35 41 36 40 38 ... ## $ compName : chr [1:360] &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; ... ## $ compConcentration: chr [1:360] &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; ... # Convert data types as needed CElegans_data$compName &lt;- factor(CElegans_data$compName, levels = unique(CElegans_data$compName)) CElegans_data$expType &lt;- factor(CElegans_data$expType, levels = unique(CElegans_data$expType)) CElegans_data$compConcentration &lt;- parse_number(CElegans_data$compConcentration) # Confirm conversion str(CElegans_data %&gt;% select(RawData, compName, compConcentration, expType)) ## tibble [360 × 4] (S3: tbl_df/tbl/data.frame) ## $ RawData : num [1:360] 44 37 45 47 41 35 41 36 40 38 ... ## $ compName : Factor w/ 5 levels &quot;2,6-diisopropylnaphthalene&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ compConcentration: num [1:360] 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 4.99 ... ## $ expType : Factor w/ 4 levels &quot;experiment&quot;,&quot;controlPositive&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 5.4 Visualizing Raw Data: Scatterplot library(ggplot2) library(tidyverse) # Plotting the scatterplot with dynamic shape assignment ggplot(CElegans_data, aes(x = log10(compConcentration), y = RawData)) + geom_point( aes(color = compName, shape = expType), size = 1.5, alpha = 0.8, position = position_jitter(width = 0.15) ) + labs( title = &quot;C. elegans Response by Compound and Concentration&quot;, caption = &quot;Raw data colored by compound, shaped by experiment type&quot;, x = &quot;log10(Compound Concentration) [nM]&quot;, y = &quot;Raw Data (counts)&quot; ) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) ## Warning: Removed 5 rows containing missing values or values outside the scale range (`geom_point()`). 5.5 Fixing Axis Label Issues in ggplot The compConcentration column was originally imported as a character variable, so ggplot2 treated it as a categorical (discrete) variable. This led to disorganized and overlapping axis labels. After converting it to numeric and applying log10(), the axis behaved as continuous, solving the issue. 5.6 Identifying Positive Controls The positive control for this experiments is controlPositive. 5.7 Identifying Negative Controls The negative control for this experiment is controlNegative. 5.8 Designing a Statistical Analysis Strategy Import and clean the dataset Convert character columns to numeric or factor types Visualize raw data for trends by concentration and compound Normalize RawData against the mean of the controlNegative group Re-plot normalized data to observe relative trends Check normality using Shapiro-Wilk tests Run ANOVA to detect significant differences across groups Apply post hoc tests if ANOVA is significant (e.g., Tukey HSD) Fit dose-response models to estimate IC50 values Compare confidence intervals between IC50 estimates Interpret results for biological and statistical significance 5.9 Normalizing Data Using the Control Group “Normalize the data for the controlNegative in such a way that the mean value for controlNegative is exactly equal to 1 and that all other values are expressed as a fraction thereof. Rerun your graphs with the normalized data.” # Calculate the mean for controlNegative neg_ctrl &lt;- CElegans_data %&gt;% filter(expType == &quot;controlNegative&quot;) neg_mean &lt;- mean(neg_ctrl$RawData) # Normalize the RawData CElegans_data &lt;- CElegans_data %&gt;% mutate(Normalized = RawData / neg_mean) # Preview normalized data head(CElegans_data %&gt;% select(compName, expType, compConcentration, RawData, Normalized)) ## # A tibble: 6 × 5 ## compName expType compConcentration RawData Normalized ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2,6-diisopropylnaphthalene experiment 4.99 44 0.512 ## 2 2,6-diisopropylnaphthalene experiment 4.99 37 0.431 ## 3 2,6-diisopropylnaphthalene experiment 4.99 45 0.524 ## 4 2,6-diisopropylnaphthalene experiment 4.99 47 0.547 ## 5 2,6-diisopropylnaphthalene experiment 4.99 41 0.477 ## 6 2,6-diisopropylnaphthalene experiment 4.99 35 0.407 ggplot(CElegans_data, aes(x = log10(compConcentration), y = Normalized)) + geom_point( aes(color = compName, shape = expType), size = 1.5, alpha = 0.8, position = position_jitter(width = 0.15) ) + labs( title = &quot;Normalized C. elegans Response&quot;, caption = &quot;Data normalized to controlNegative (mean = 1)&quot;, x = &quot;log10(Compound Concentration) [nM]&quot;, y = &quot;Normalized Raw Data&quot; ) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) ## Warning: Removed 5 rows containing missing values or values outside the scale range (`geom_point()`). 5.10 Why Normalize? This normalization allows for clearer comparison of relative differences between conditions. By setting the control to 1, the effect size of each treatment becomes easier to interpret. "],["reproducable-research-part-1.html", "6 Reproducable research part 1 6.1 Heterologous prime-boost immunisation with mRNA- and AdC68-based 2019-nCoV variant vaccines induces broad-spectrum immune responses in mice 6.2 Estimating the effects of non-pharmaceutical interventions on COVID-19 in Europe", " 6 Reproducable research part 1 6.1 Heterologous prime-boost immunisation with mRNA- and AdC68-based 2019-nCoV variant vaccines induces broad-spectrum immune responses in mice Xingxing Li, Jingjing Liu, Wenjuan Li, Qinhua Peng, Miao Li, Zhifang Ying, Zelun Zhang, Xinyu Liu, Xiaohong Wu, Danhua Zhao, Lihong Yang, Shouchun Cao, Yanqiu Huang, Leitai Shi, Hongshan Xu, Yunpeng Wang, Guangzhi Yue, Yue Suo, Jianhui Nie, Weijin Huang, Jia Li, * and Yuhua Li. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10050358/ (add reference later) Add research question Add summary methode en resultaten #Reproducable research part 2 6.1.1 Study purpose There was no concise statement in the introduction of the article, that established the reason the research was conducted. 6.1.2 Study location The study location has been mentioned in the methods, it was conducted at National institute for food and drug control (NIFDC). 6.1.3 Ethics statement There is a small ethics statement on handling of the animals and a small section on conflict of interest, this states there have been no conflicts with any other party. 6.1.4 Funding statement There is no section on fundings, however there is a section on acknowledgements. Here 2 companies are acknowledged for the provision of vaccines, it is rather unclear if these were provided for free or bought. 6.1.5 Data availability statement and location There is a small section where supplementary materials can be accessed. THere is a download link to the ‘data sheet’. The sheet is a word doc with the processed data in figures in a doc file. Access to raw data would be better to have. It does however mention that you can inquire for more data but it requires you to contact the authors but, there is no author contact info available. Furthermore no code was used for this research. #Reproducable research part 3 6.2 Estimating the effects of non-pharmaceutical interventions on COVID-19 in Europe (https://www.nature.com/articles/s41586-020-2405-7) The paper has R code shared in the project environment and has clear instructions how to reproduce the data from the paper. (Describe the code:) The script runs smoohtly after installing all packages. I got the code from : https://github.com/ImperialCollegeLondon/covid19model.git It clearly states version 7 was used for the Nature paper so I used that one. For this base-nature.r script was used. On a scale from 1-5 I would rate this paper a 5 in how easy it was to reproduce. I could import the all the scripts from their github and rerun their data through the scripts the used. library(rstan) library(data.table) library(lubridate) library(gdata) library(dplyr) library(tidyr) library(EnvStats) library(optparse) library(stringr) library(bayesplot) library(matrixStats) library(scales) library(gridExtra) library(ggpubr) library(cowplot) library(ggplot2) library(abind) source(&#39;~/Portfolio-AlexS/ReproducibleArticleFiles/process-covariates.r&#39;) # Commandline options and parsing parser &lt;- OptionParser() parser &lt;- add_option(parser, c(&quot;-D&quot;, &quot;--debug&quot;), action=&quot;store_true&quot;, help=&quot;Perform a debug run of the model&quot;) parser &lt;- add_option(parser, c(&quot;-F&quot;, &quot;--full&quot;), action=&quot;store_true&quot;, help=&quot;Perform a full run of the model&quot;) cmdoptions &lt;- parse_args(parser, args = commandArgs(trailingOnly = TRUE), positional_arguments = TRUE) # Default run parameters for the model if(is.null(cmdoptions$options$debug)) { DEBUG = Sys.getenv(&quot;DEBUG&quot;) == &quot;TRUE&quot; } else { DEBUG = cmdoptions$options$debug } # Sys.setenv(FULL = &quot;TRUE&quot;) if(is.null(cmdoptions$options$full)) { FULL = Sys.getenv(&quot;FULL&quot;) == &quot;TRUE&quot; } else { FULL = cmdoptions$options$full } if(DEBUG &amp;&amp; FULL) { stop(&quot;Setting both debug and full run modes at once is invalid&quot;) } if(length(cmdoptions$args) == 0) { StanModel = &#39;base-nature&#39; } else { StanModel = cmdoptions$args[1] } print(sprintf(&quot;Running %s&quot;,StanModel)) if(DEBUG) { print(&quot;Running in DEBUG mode&quot;) } else if (FULL) { print(&quot;Running in FULL mode&quot;) } cat(sprintf(&quot;Running:\\nStanModel = %s\\nDebug: %s\\n&quot;, StanModel,DEBUG)) # Read which countires to use countries &lt;- readRDS(&#39;~/Portfolio-AlexS/ReproducibleArticleFiles/regions.rds&#39;) # Read deaths data for regions d &lt;- readRDS(&#39;~/Portfolio-AlexS/ReproducibleArticleFiles/COVID-19-up-to-date.rds&#39;) # Read IFR and pop by country ifr.by.country &lt;- readRDS(&#39;~/Portfolio-AlexS/ReproducibleArticleFiles/popt-ifr.rds&#39;) # Read interventions interventions &lt;- readRDS(&#39;~/Portfolio-AlexS/ReproducibleArticleFiles/interventions.rds&#39;) forecast &lt;- 0 # increase to get correct number of days to simulate # Maximum number of days to simulate N2 &lt;- (max(d$DateRep) - min(d$DateRep) + 1 + forecast)[[1]] processed_data &lt;- process_covariates(countries = countries, interventions = interventions, d = d , ifr.by.country = ifr.by.country, N2 = N2) stan_data = processed_data$stan_data dates = processed_data$dates deaths_by_country = processed_data$deaths_by_country reported_cases = processed_data$reported_cases options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) m = stan_model(paste0(&#39;~/Portfolio-AlexS/ReproducibleArticleFiles/&#39;,StanModel,&#39;.stan&#39;)) if(DEBUG) { fit = sampling(m,data=stan_data,iter=40,warmup=20,chains=2) } else if (FULL) { fit = sampling(m,data=stan_data,iter=1800,warmup=1000,chains=5,thin=1,control = list(adapt_delta = 0.99, max_treedepth = 20)) } else { fit = sampling(m,data=stan_data,iter=600,warmup=300,chains=4,thin=1,control = list(adapt_delta = 0.95, max_treedepth = 10)) } out = rstan::extract(fit) prediction = out$prediction estimated.deaths = out$E_deaths estimated.deaths.cf = out$E_deaths0 JOBID = Sys.getenv(&quot;PBS_JOBID&quot;) if(JOBID == &quot;&quot;) JOBID = as.character(abs(round(rnorm(1) * 1000000))) print(sprintf(&quot;Jobid = %s&quot;,JOBID)) countries &lt;- countries$Regions save(fit,prediction,dates,reported_cases,deaths_by_country,countries,estimated.deaths,estimated.deaths.cf,stan_data, file=paste0(&#39;~/Portfolio-AlexS/ReproducibleArticleFiles/Results/&#39;,StanModel,&#39;-&#39;,JOBID,&#39;-stanfit.Rdata&#39;)) library(bayesplot) filename &lt;- paste0(StanModel,&#39;-&#39;,JOBID) print(&#39;Generating mu, rt plots&#39;) mu = (as.matrix(out$mu)) colnames(mu) = countries g = (mcmc_intervals(mu,prob = .9)) ggsave(sprintf(&quot;~/Portfolio-AlexS/ReproducibleArticleFiles/%s-mu.png&quot;,filename),g,width=4,height=6) tmp = lapply(1:length(countries), function(i) (out$Rt_adj[,stan_data$N[i],i])) Rt_adj = do.call(cbind,tmp) colnames(Rt_adj) = countries g = (mcmc_intervals(Rt_adj,prob = .9)) ggsave(sprintf(&quot;~/Portfolio-AlexS/ReproducibleArticleFiles/%s-final-rt.png&quot;,filename),g,width=4,height=6) print(&quot;Generate 3-panel plots&quot;) source(&quot;~/Portfolio-AlexS/ReproducibleArticleFiles/plot-3-panel.r&quot;) make_three_panel_plot(filename) print(&#39;Covars plots&#39;) source(&#39;~/Portfolio-AlexS/ReproducibleArticleFiles/covariate-size-effects.r&#39;) plot_covars(filename) print(&#39;Making table&#39;) source(&#39;~/Portfolio-AlexS/ReproducibleArticleFiles/make-table.r&#39;) make_table(filename) "],["relational-databases.html", "7 Relational databases", " 7 Relational databases Passed the SQL assesment in Period D of academic year of 2023/2024 "],["creating-r-package.html", "8 Creating R package", " 8 Creating R package "],["variant-analysis-tools.html", "9 Variant Analysis tools 9.1 Summary personal Timeline and Motivation 9.2 Background 9.3 Discussion 9.4 Conclusion and Recommendations", " 9 Variant Analysis tools 9.1 Summary personal Timeline and Motivation My initial plan was to perform a comprehensive comparison between vcfR, VariantAnnotation, and VariantTools. However, this proved impractical due to incompatible input requirements (VCF vs BAM). I dropped VariantTools from the initial comparison and refocused on filtering using vcfR and VariantAnnotation. My large VCF dataset crashed vcfR, while VariantAnnotation managed to process it. I attempted smaller datasets but faced issues with missing or poor quality scores. I experimented with deepSNV and mitoClone2 for variant calling but encountered persistent file access errors. Revisited VariantTools for variant calling but was blocked by gmapR/GSNAP issues on Windows. Pivoted to using the VariantAnnotation tutorial dataset (chromosome 22) and compared it with vcfR for variant filtering. Throughout this process, my choices were guided by tool limitations, computational feasibility, and my evolving understanding of each package’s strengths. 9.2 Background Variant calling is a crucial step in genomic analysis that involves identifying differences in DNA sequence compared to a reference genome. These variants can include single nucleotide polymorphisms (SNPs), insertions, deletions, and more. Accurate variant calling is essential for downstream applications such as disease gene discovery, pharmacogenomics, and population genetics. Several R packages are available for handling variant data, each with its own strengths and limitations: VariantAnnotation: A comprehensive Bioconductor package that provides functionality for reading, annotating, and filtering VCF files. It’s particularly strong in structured data handling and integration with annotation resources. vcfR: Designed for fast and interactive VCF file exploration, especially useful for lightweight operations, visualization, and subsetting. VariantTools: An end-to-end framework for variant calling and filtering directly from BAM files, integrated with Bioconductor’s genomic data structures. deepSNV: A package tailored to identifying low-frequency variants, particularly useful for ultra-deep sequencing data. mitoClone2: A downstream tool based on deepSNV, focused on mitochondrial heteroplasmy and clonal lineage analysis. I initially set out to perform a three-way comparison between the R packages VariantAnnotation, vcfR, and VariantTools. My goal was to compare how these tools handled variant filtering, annotation, and representation. However, I quickly discovered that the input formats and assumptions across the packages were not aligned. VariantTools expects BAM files, while vcfR and VariantAnnotation work directly with VCFs. Due to this, I explored deepSNV. Unfortunately, this failed due to file access issues when trying to extract base counts from BAM files: library(deepSNV) bam_list &lt;- list(H1993 = &quot;H1993.bam&quot;) tp53_region &lt;- GenomicRanges::GRanges(&quot;chr17&quot;, IRanges(7565097, 7590856)) counts &lt;- baseCountsFromBamList(bamfiles = bam_list, sites = tp53_region) This motivated me to try mitoClone2, which builds on deepSNV, but I encountered the same BAM access issues: library(mitoClone2) bam_list &lt;- list(H1993 = &quot;H1993.bam&quot;) tp53_region &lt;- GenomicRanges::GRanges(&quot;chr17&quot;, IRanges(7565097, 7590856)) counts &lt;- baseCountsFromBamList(bamfiles = bam_list, sites = tp53_region) Since both packages depend heavily on deepSNV’s functionality and required very specific BAM setups, I dropped them. However, I recognize that deepSNV is suitable for detecting low-frequency variants in ultra-deep sequencing, and mitoClone2 would be well-suited for mitochondrial DNA analysis and clonal structure inference in single-cell data. Simultaneously, I explored using VariantTools again for variant calling. But I discovered that subsetting data with this package depends on gmapR, which in turn relies on GSNAP. This made it incompatible with Windows, making my plan to use a smaller subset of data infeasible within my current setup. Given the challenges with variant calling, I returned to my original idea of variant filtering. I aimed to compare vcfR and VariantAnnotation more thoroughly. I started with a large VCF file. VariantAnnotation could process it, but vcfR repeatedly crashed my system due to memory limitations: library(VariantAnnotation) vcf_obj &lt;- readVcf(&quot;large_sample.vcf.gz&quot;, &quot;hg38&quot;) rowRanges(vcf_obj) library(vcfR) vcf &lt;- read.vcfR(&quot;large_sample.vcf.gz&quot;) # Crashed system To deal with this, I shifted to a smaller dataset. Unfortunately, I then found that the data either had no QUAL scores or all values were below 30, making them unsuitable for quality-based filtering. I decided to pivot again to well-characterized, smaller datasets—specifically, those based on chromosomes 20 or 22, which are often used in tutorials due to their manageable size and representative features. I initially downloaded a chromosome 22 dataset, but it still proved too large. Eventually, I found that VariantAnnotation includes a built-in example dataset based on chromosome 22. This dataset allowed me to continue my comparison without crashing or compatibility issues. library(VariantAnnotation) fl &lt;- system.file(&quot;extdata&quot;, &quot;chr22.vcf.gz&quot;, package = &quot;VariantAnnotation&quot;) vcf &lt;- readVcf(fl, &quot;hg19&quot;) summary(vcf) This final adjustment allowed me to complete my comparative analysis between vcfR and VariantAnnotation effectively, using realistic and tractable data. 9.3 Discussion This project revealed both the power and the constraints of variant analysis tools in R. While my initial goals were ambitious, real-world file sizes, system constraints, and software compatibility forced me to adapt repeatedly. 9.3.1 What Went Well Successfully explored multiple R packages and understood their input/output structures. Managed to compare and visualize variant calls using GenomicRanges, even without full pipeline compatibility. Gained valuable insights into what tools are realistic to run on a typical local setup. 9.3.2 What Didn’t Work deepSNV and mitoClone2 failed due to BAM access issues and setup requirements. VariantTools’ reliance on gmapR made it unusable on Windows. Large datasets were infeasible to process with vcfR due to memory limits. 9.4 Conclusion and Recommendations Based on my experience: Use vcfR for quick, small-scale variant filtering and visualization. It’s great for exploratory analysis but not built for large files. Use VariantAnnotation for structured, reproducible workflows and integration with annotation databases. It handles large files more gracefully and provides robust filtering tools. Avoid VariantTools on Windows unless working in a Unix-based environment with GSNAP installed. Avoid deepSNV and mitoClone2 unless you have a very specific need for ultra-deep sequencing or mitochondrial variant analysis and your environment is set up accordingly. This project has been a challenging but rewarding exploration of the practicalities behind variant analysis in R. It highlighted the importance of choosing tools that align with both data and computing resources. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
